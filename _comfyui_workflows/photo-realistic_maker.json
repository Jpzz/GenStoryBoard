{
  "1": {
    "inputs": {
      "ckpt_name": "flux\\flux1-dev-fp8.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "2": {
    "inputs": {
      "shot": "Head and shoulders portrait",
      "shot_weight": 1.3500000000000003,
      "gender": "Woman",
      "androgynous": 0,
      "ugly": 0,
      "ordinary_face": 0,
      "age": "22",
      "nationality_1": "British",
      "nationality_2": "-",
      "nationality_mix": 0.5000000000000001,
      "body_type": "Slight",
      "body_type_weight": 1.0000000000000002,
      "eyes_color": "random ðŸŽ²",
      "eyes_shape": "random ðŸŽ²",
      "lips_color": "random ðŸŽ²",
      "lips_shape": "random ðŸŽ²",
      "facial_expression": "-",
      "facial_expression_weight": 1.0000000000000002,
      "face_shape": "Heart with V-Shape Chin",
      "face_shape_weight": 1.0000000000000002,
      "facial_asymmetry": 0,
      "hair_style": "random ðŸŽ²",
      "hair_color": "Auburn",
      "hair_length": "Medium",
      "disheveled": 0,
      "beard": "-",
      "beard_color": "-",
      "active": true
    },
    "class_type": "PortraitMasterBaseCharacter",
    "_meta": {
      "title": "Portrait Master: Base Character"
    }
  },
  "3": {
    "inputs": {
      "natural_skin": 0,
      "bare_face": 0,
      "washed_face": 0,
      "dried_face": 0,
      "skin_details": 1.0000000000000002,
      "skin_pores": 0,
      "dimples": 0,
      "wrinkles": 0,
      "freckles": 0,
      "moles": 0,
      "skin_imperfections": 0,
      "skin_acne": 0,
      "tanned_skin": 0,
      "eyes_details": 1.0000000000000002,
      "iris_details": 0,
      "circular_iris": 0,
      "circular_pupil": 0,
      "active": true,
      "text_in": [
        "2",
        0
      ]
    },
    "class_type": "PortraitMasterSkinDetails",
    "_meta": {
      "title": "Portrait Master: Skin Details"
    }
  },
  "4": {
    "inputs": {
      "model_pose": "Headshot Pose",
      "clothes": "Casual Dress",
      "female_lingerie": "-",
      "makeup": "Classic Makeup",
      "light_type": "Ambient Light",
      "light_direction": "Light from top-left",
      "light_weight": 1.0000000000000002,
      "style_1": "-",
      "style_1_weight": 1.0000000000000002,
      "style_2": "-",
      "style_2_weight": 1.0000000000000002,
      "photorealism_improvement": true,
      "active": true,
      "text_in": [
        "3",
        0
      ]
    },
    "class_type": "PortraitMasterStylePose",
    "_meta": {
      "title": "Portrait Master: Style & Pose"
    }
  },
  "5": {
    "inputs": {
      "makeup_style": "Natural Makeup",
      "makeup_color": "-",
      "eyeshadow": false,
      "eyeliner": false,
      "mascara": false,
      "blush": false,
      "lipstick": false,
      "lip_gloss": false,
      "active": true,
      "text_in": [
        "4",
        0
      ]
    },
    "class_type": "PortraitMasterMakeup",
    "_meta": {
      "title": "Portrait Master: Make-up"
    }
  },
  "9": {
    "inputs": {
      "delimiter": ",",
      "string1": [
        "5",
        0
      ],
      "string2": [
        "48",
        0
      ]
    },
    "class_type": "JoinStrings",
    "_meta": {
      "title": "Join Strings"
    }
  },
  "11": {
    "inputs": {
      "seed": 559864780198793,
      "steps": 30,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 1,
      "model": [
        "47",
        0
      ],
      "positive": [
        "13",
        0
      ],
      "negative": [
        "12",
        0
      ],
      "latent_image": [
        "15",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler_1st"
    }
  },
  "12": {
    "inputs": {
      "text": "watermark, text, nsfw, nude, deformed, glitch, noisy, braids, naked, reflection, hat, cap, accessories, flowers, error, glow, low quality, nsfw, nude, naked, mole, earings",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Negative Prompt"
    }
  },
  "13": {
    "inputs": {
      "guidance": 4,
      "conditioning": [
        "14",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "FluxGuidance"
    }
  },
  "14": {
    "inputs": {
      "text": [
        "9",
        0
      ],
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Positive Prompt"
    }
  },
  "15": {
    "inputs": {
      "dimensions": " 896 x 1152  (portrait)",
      "clip_scale": 1,
      "batch_size": 1
    },
    "class_type": "SDXL Empty Latent Image (rgthree)",
    "_meta": {
      "title": "SDXL Empty Latent Image (rgthree)"
    }
  },
  "16": {
    "inputs": {
      "samples": [
        "11",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "17": {
    "inputs": {
      "control_net_name": "FLUX.1\\jasperai-dev-Upscaler\\diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "18": {
    "inputs": {
      "strength": 0.7000000000000002,
      "start_percent": 0,
      "end_percent": 0.7000000000000002,
      "positive": [
        "13",
        0
      ],
      "negative": [
        "12",
        0
      ],
      "control_net": [
        "17",
        0
      ],
      "image": [
        "16",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "19": {
    "inputs": {
      "model_name": "4x_NickelbackFS_72000_G.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "20": {
    "inputs": {
      "upscale_model": [
        "19",
        0
      ],
      "image": [
        "16",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "Upscale Image (using Model)"
    }
  },
  "21": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "scale_by": 0.5000000000000001,
      "image": [
        "20",
        0
      ]
    },
    "class_type": "ImageScaleBy",
    "_meta": {
      "title": "Upscale Image By"
    }
  },
  "22": {
    "inputs": {
      "seed": 228131653784800,
      "steps": 20,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 0.20000000000000004,
      "model": [
        "47",
        0
      ],
      "positive": [
        "18",
        0
      ],
      "negative": [
        "18",
        1
      ],
      "latent_image": [
        "23",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler_2nd"
    }
  },
  "23": {
    "inputs": {
      "pixels": [
        "21",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "24": {
    "inputs": {
      "samples": [
        "22",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "25": {
    "inputs": {
      "ckpt_name": "sd15\\insaneRealisticV20_insaneRealisticV20.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "26": {
    "inputs": {
      "pixels": [
        "39",
        0
      ],
      "vae": [
        "25",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "27": {
    "inputs": {
      "seed": 135435853376849,
      "steps": 25,
      "cfg": 7,
      "sampler_name": "dpmpp_2m",
      "scheduler": "sgm_uniform",
      "denoise": 0.10000000000000002,
      "model": [
        "25",
        0
      ],
      "positive": [
        "29",
        0
      ],
      "negative": [
        "81",
        0
      ],
      "latent_image": [
        "26",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler_3rd"
    }
  },
  "29": {
    "inputs": {
      "text": [
        "9",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Positive Prompt"
    }
  },
  "30": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "scale_by": 0.5000000000000001,
      "image": [
        "31",
        0
      ]
    },
    "class_type": "ImageScaleBy",
    "_meta": {
      "title": "Upscale Image By"
    }
  },
  "31": {
    "inputs": {
      "samples": [
        "27",
        0
      ],
      "vae": [
        "25",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "32": {
    "inputs": {
      "images": [
        "30",
        0
      ]
    },
    "class_type": "SaveImageWebsocket",
    "_meta": {
      "title": "SaveImageWebsocket"
    }
  },
  "39": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 129572122858298,
      "steps": 20,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 0.20000000000000004,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5000000000000001,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.9300000000000002,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7000000000000002,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "tiled_encode": false,
      "tiled_decode": false,
      "image": [
        "24",
        0
      ],
      "model": [
        "47",
        0
      ],
      "clip": [
        "1",
        1
      ],
      "vae": [
        "1",
        2
      ],
      "positive": [
        "13",
        0
      ],
      "negative": [
        "12",
        0
      ],
      "bbox_detector": [
        "46",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "46": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "47": {
    "inputs": {
      "object_to_patch": "diffusion_model",
      "residual_diff_threshold": 0.12000000000000002,
      "start": 0,
      "end": 0.6000000000000001,
      "max_consecutive_cache_hits": -1,
      "model": [
        "1",
        0
      ]
    },
    "class_type": "ApplyFBCacheOnModel",
    "_meta": {
      "title": "Apply First Block Cache"
    }
  },
  "48": {
    "inputs": {
      "string": "clear black background, sharp, not blurry, (deep focus:1.1), No mouth wrinkles, studio lighting, photoreal, hyper real, ultra detail, 4k, Skin blemish, detailed skin, perfect eyes, 20221011_studio_photo.jpg"
    },
    "class_type": "String Literal (Image Saver)",
    "_meta": {
      "title": "String Literal (Image Saver)"
    }
  },
  "81": {
    "inputs": {
      "text": "watermark, text, nsfw, nude, deformed, glitch, noisy, braids, naked, reflection, hat, cap, accessories, flowers, error, glow, low quality, nsfw, nude, naked, mole, earings"
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Negative Prompt"
    }
  },
  "91": {
    "inputs": {
      "images": [
        "16",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "93": {
    "inputs": {
      "images": [
        "21",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "95": {
    "inputs": {
      "images": [
        "24",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "97": {
    "inputs": {
      "images": [
        "39",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "99": {
    "inputs": {
      "images": [
        "31",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  }
}